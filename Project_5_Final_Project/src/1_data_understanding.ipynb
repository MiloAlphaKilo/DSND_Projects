{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pycharm-5a24c1e6",
   "language": "python",
   "display_name": "PyCharm (udacity)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data Analysis\n",
    "\n",
    "#### This notebook will be used for the analysis part of the project. Here we will explore and visualise the data to get a firm grasp on what the data looks like.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "source": [
    "### A tale of two data sets\n",
    "\n",
    "Two sets of data will be used in this project. \n",
    "\n",
    "1. The set of organisations fulfilled by the [IPEDS Data base](https://nces.ed.gov/ipeds/)\n",
    "2. The customer behavior is a ficticious data set creating random numbers and ranges. Fingers crossed we have some correlation in the data. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data to frame\n",
    "\n",
    "# Import IPDES data\n",
    "# The data comes in the form of csv\n",
    "df_organisations_raw = pd.read_csv(\"../data/hd2019.csv\", encoding='latin1') #encoding had to be added to be able to read the file\n",
    "# Import the customer behaviour data\n",
    "df_customer_data_raw = pd.read_csv(\"../data/organisation_sales_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data shape \n",
    "\n",
    "raw_org_columns = df_organisations_raw.shape[1]\n",
    "raw_org_rows = df_organisations_raw.shape[0]\n",
    "\n",
    "# Manual identification of columns that will have values \n",
    "columns_of_value = ['UNITID', 'INSTNM', 'IALIAS', 'CITY', 'STABBR', 'FIPS', 'OBEREG','GENTELE', 'EIN', 'DUNS', 'OPEID', \n",
    "                    'OPEFLAG', 'SECTOR', 'ICLEVEL', 'CONTROL', 'HLOFFER', 'UGOFFER','GROFFER', 'HDEGOFR1', 'DEGGRANT', \n",
    "                    'HBCU', 'HOSPITAL', 'MEDICAL', 'TRIBAL', 'LOCALE', 'OPENPUBL', 'ACT', 'DEATHYR', 'CYACTIVE', \n",
    "                    'POSTSEC', 'PSEFLAG', 'PSET4FLG', 'RPTMTH', 'INSTCAT', 'C18BASIC', 'C18IPUG', 'C18IPGRD', 'C18UGPRF', \n",
    "                    'C18ENPRF', 'C18SZSET', 'C15BASIC', 'CCBASIC', 'CARNEGIE', 'LANDGRNT', 'INSTSIZE', 'F1SYSTYP', \n",
    "                    'F1SYSCOD', 'COUNTYNM', 'CNGDSTCD']\n",
    "\n",
    "df_organisations = df_organisations_raw[columns_of_value]\n",
    "\n",
    "# Testing for columns that aren't ranges (categorical)\n",
    "# cust_cols_of_value = ['UNITID', 'CONVERTED', 'DID_TRIAL', 'PURCHASED_PREVIOUS_PRODUCT','AMOUNT_OF_LICENSES',\n",
    "#                       'TERM_OF_LICENSE', 'HAS_TECH_DEPT', 'AMOUNT_OF_INTERACTIONS_W_SALES', 'AMOUNT_OF_CALLS', \n",
    "#                       'AMOUNT_OF_MESSAGES', 'ENGAGED_WITH_MESSAGING', 'REACHED_NOT_ENGAGED_WITH_MESSAGING', \n",
    "#                       'ATTENDED_WEBINARS', 'WEBINAR_ATTENDANCE_SIZE']\n",
    "\n",
    "cust_cols_of_value = ['UNITID', 'CONTACTED', 'CONVERTED', 'DID_TRIAL', 'PURCHASED_PREVIOUS_PRODUCT','AMOUNT_OF_LICENSES',\n",
    "                      'AMOUNT_OF_LICENSES_RNG', 'TERM_OF_LICENSE', 'TERM_OF_LICENSE_RNG', 'HAS_TECH_DEPT', \n",
    "                      'AMOUNT_OF_INTERACTIONS_W_SALES', 'AMOUNT_OF_INTERACTIONS_W_SALES_RNG','AMOUNT_OF_CALLS', \n",
    "                      'AMOUNT_OF_CALLS_RNG', 'AMOUNT_OF_MESSAGES', 'AMOUNT_OF_MESSAGES_RNG', 'ENGAGED_WITH_MESSAGING', \n",
    "                      'REACHED_NOT_ENGAGED_WITH_MESSAGING', 'ATTENDED_WEBINARS', 'WEBINAR_ATTENDANCE_SIZE', \n",
    "                      'WEBINAR_ATTENDANCE_SIZE_RNG']\n",
    "\n",
    "df_customer_data = df_customer_data_raw[cust_cols_of_value]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset to remove records that arent relevant\n",
    "# Remove institutions that have closed down  or removed from IPEDS using column CLOSEDAT & DEATHYR\n",
    "\n",
    "df_organisations_filtered = df_organisations.loc[(df_organisations['DEATHYR']==-2)]"
   ]
  },
  {
   "source": [
    "### Uderstanding the data\n",
    "* Get the shape of the data\n",
    "* Identify columns with categorical values\n",
    "* Visualise the organisations by state"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understading the data\n",
    "\n",
    "raw_columns = df_organisations_raw.shape[1]\n",
    "raw_rows = df_organisations_raw.shape[0]\n",
    "org_columns = df_organisations.shape[1]\n",
    "org_rows = df_organisations_raw.shape[0]\n",
    "fil_org_columns = df_organisations_filtered.shape[1]\n",
    "fil_org_rows = df_organisations_filtered.shape[0]\n",
    "is_nulls_counts = len(df_organisations_filtered.columns[df_organisations_filtered.isnull().sum()==0])\n",
    "categorical_vals = df_organisations_filtered.select_dtypes(include=['object'])\n",
    "categorical_vals_columns = list(categorical_vals.columns)\n",
    "\n",
    "print(f'The raw data has {raw_columns} columns and {raw_rows} rows')\n",
    "print(f'The unfiltered reduced data has {org_columns} columns and {org_rows} rows')\n",
    "print(f'The filtered organisations df has {fil_org_columns} columns and {fil_org_rows} rows')\n",
    "print(f'The df has {df_organisations_filtered.shape[1]} columns without {is_nulls_counts} containing nulls')\n",
    "print(f'There are {categorical_vals.shape[1]} columns with categorical values, they are in the columns: '\n",
    "      f'\\n\\n{categorical_vals_columns}')\n"
   ]
  },
  {
   "source": [
    "## The distribution of organisations across states\n",
    "The Top 5 States with organisations: \n",
    "CA, NY, FL, PA, TX"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_organisations_filtered['STABBR'].hist(bins=55, grid=False, figsize=(40, 15), xlabelsize=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_organisations_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cust_raw_columns = df_customer_data.shape[1]\n",
    "cust_raw_rows = df_customer_data.shape[0]\n",
    "cust_is_nulls_counts = len(df_customer_data.columns[df_customer_data.isnull().sum()==0])\n",
    "cust_categorical_vals = df_customer_data.select_dtypes(include=['object'])\n",
    "cust_categorical_vals_columns = list(cust_categorical_vals.columns)\n",
    "\n",
    "print(f'The raw customer data has {cust_raw_columns} columns and {cust_raw_rows} rows')\n",
    "print(f'The customer df has {cust_is_nulls_counts} columns without null values')\n",
    "print(f'There are {cust_categorical_vals.shape[1]} categorical values in the customer data set. they are in the columns: '\n",
    "      f'\\n\\n{cust_categorical_vals_columns}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "source": [
    "### Combining the data sets with each other. \n",
    "\n",
    "Using the `UNITID` column we will joing the data sets before looking for correlation in the features"
   ],
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_data = df_organisations_filtered.merge(df_customer_data, on='UNITID', how='left')\n",
    "\n",
    "df_combined_data.head()"
   ]
  },
  {
   "source": [
    "#### Replace `-2` and `-1` values in the data set\n",
    "The IPEDS database uses `-2` and `-1` to indicate `null` or empty values, this might cause issues when applying the segmentation logic. \n",
    "\n",
    "At this point I will replace all `[-2, -1]` values with `0`. The data set uses `[-2, -1]` in the context of `integer` and `string` values so both need to be accomodated"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remove_ambiguous_values = df_combined_data.replace([-2, '-2', -1, '-1'], 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dummy df for correlations analysis\n",
    "\n",
    "df_dummies = pd.get_dummies(df_customer_data, prefix='d', prefix_sep='_', drop_first=True, dummy_na='dummy_na')\n",
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add prefix to the range fields for better view/understanding of correlation. \n",
    "\n",
    "col_prefix = {'AMOUNT_OF_LICENSES_RNG': 'lic_', 'TERM_OF_LICENSE_RNG': 'trm_', 'AMOUNT_OF_INTERACTIONS_W_SALES_RNG': 'int_', \n",
    "              'AMOUNT_OF_CALLS_RNG': 'cls_', 'AMOUNT_OF_MESSAGES_RNG': 'msg_', 'WEBINAR_ATTENDANCE_SIZE_RNG': 'web_'}\n",
    "\n",
    "df = df_customer_data.copy()\n",
    "\n",
    "for col, prefix in col_prefix.items(): \n",
    "    df[col] = prefix + df[col]\n",
    "\n",
    "df_dummies_ranges = pd.get_dummies(df, prefix='d', prefix_sep='_', drop_first=True, dummy_na='dummy_na')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1)\n",
    "plt.subplots(figsize=(20,10))\n",
    "sns.heatmap(df_dummies_ranges.corr(), annot=False, linewidths=.1, cmap=\"YlGnBu\", fmt=\".2f\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "plt.subplots(figsize=(20,10))\n",
    "sns.heatmap(df_customer_data_raw.corr(), annot=False, linewidths=.1, cmap=\"YlGnBu\", fmt=\".2f\");"
   ]
  },
  {
   "source": [
    "## Interpretation of correlation analysis\n",
    "\n",
    "The correlation is superficial at best. From this we can make redementary deductions like: \n",
    "\n",
    "* There is a correlation between conversion and the amount of licenses bought;\n",
    "* There is a correlation between calls/messages and sales interactions; and\n",
    "* Webinar attendance and size of attendance\n",
    "\n",
    "This is unfortunately due to the data being generated with random generators. \n",
    "\n",
    "What is interesting is that there seems to be an inverse correlation between messages and calls. Which could suggest that there more there is of the one the less there is needed of the other. It doesn't however show us there is/isn't a correlation between sales interactions and conversions. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output data for next module to sqlite db\n",
    "df_data_for_preparetion = df_remove_ambiguous_values.copy()\n",
    "engine = create_engine('sqlite:///../data/customers_with_behaviours.db')\n",
    "df_data_for_preparetion.to_sql('customers_with_behaviours', engine, if_exists='replace', index=False)"
   ]
  }
 ]
}