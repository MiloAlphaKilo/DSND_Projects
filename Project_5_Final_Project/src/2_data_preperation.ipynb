{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Data preparation\n",
    "\n",
    "Before the data can be sent through the KNN model it needs to be tidied. \n",
    "Here I will: \n",
    "\n",
    "* Remove nulls;\n",
    "* Create dummies as required;\n",
    "* Normalise values to prevent exponential scales;\n",
    "* Further reduce the data set to only relevant classifiers. \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create connections to data and read data to pandas dataframe\n",
    "engine = create_engine('sqlite:///../data/customers_with_behaviours.db')\n",
    "df_initial = pd.read_sql_table('customers_with_behaviours', engine)\n",
    "df_initial.head()"
   ]
  },
  {
   "source": [
    "### Introducing additional data for features and or classifiers do the data set. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ic2019 = pd.read_csv('../data/ic2019.csv')\n",
    "df_adm2019 = pd.read_csv('../data/adm2019.csv')"
   ]
  },
  {
   "source": [
    "### Reduce the files to only the required columns. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ic2019_cls_col = ['UNITID', 'PEO1ISTR', 'PEO2ISTR', 'PEO3ISTR', 'PEO4ISTR', 'PEO5ISTR', 'PEO6ISTR', 'CNTLAFFI',                            'PUBPRIME', 'PUBSECON', 'RELAFFIL', 'LEVEL1', 'LEVEL2', 'LEVEL3', 'LEVEL4', 'LEVEL5', 'LEVEL6',                          'LEVEL7', 'LEVEL8', 'LEVEL12', 'LEVEL17', 'LEVEL18', 'LEVEL19', 'CALSYS', 'FT_UG', 'FT_FTUG',                            'FTGDNIDP', 'PT_UG', 'PT_FTUG', 'PTGDNIDP', 'DOCPP', 'DOCPPSP', 'OPENADMP', 'CREDITS1', 'CREDITS2',                      'CREDITS3', 'CREDITS4', 'STUSRV2', 'STUSRV3', 'STUSRV4', 'STUSRV8', 'LIBRES1', 'LIBRES2', 'LIBRES3',                     'LIBRES4', 'LIBRES5', 'TUITPL', 'TUITPL1', 'TUITPL2', 'TUITPL3', 'TUITPL4', 'DSTNUGC', 'DSTNUGP',                        'DSTNUGN', 'DSTNGC', 'DSTNGP', 'DSTNGN', 'DISTCRS', 'DISTPGS', 'DSTNCED1', 'DSTNCED2', 'DSTNCED3',                       'DISTNCED', 'DISAB', 'ROOM', 'ROOMCAP', 'BOARD']\n",
    "\n",
    "df_cls_ic2019 = df_ic2019[df_ic2019_cls_col]\n",
    "\n",
    "df_adm2019_cls_cols = ['UNITID', 'APPLCN', 'APPLCNM', 'APPLCNW', 'ADMSSN', 'ADMSSNM', 'ADMSSNW', 'ENRLT', 'ENRLM',\n",
    "                       'ENRLW', 'ENRLFT', 'ENRLFTM', 'ENRLFTW', 'ENRLPT', 'ENRLPTM', 'ENRLPTW', 'SATNUM', 'SATPCT', \n",
    "                       'ACTNUM', 'ACTPCT', 'SATVR25', 'SATVR75', 'SATMT25', 'SATMT75']\n",
    "\n",
    "df_cls_adm2019 = df_adm2019[df_adm2019_cls_cols]"
   ]
  },
  {
   "source": [
    "### Join the data sets together"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_features = df_cls_adm2019.merge(df_cls_ic2019, on='UNITID') #, how='left')\n",
    "df_all_data = df_initial.merge(df_combined_features, on='UNITID') #, how='left')\n",
    "print(f\"\\nThe dataframe df_all_data is shaped with {df_all_data.shape[1]} columns and {df_all_data.shape[0]} rows\\n\\n\")\n",
    "print(f\"Here is the head of the dataframe ... \\n {df_all_data.head()}\")"
   ]
  },
  {
   "source": [
    "#### Here we will remove the `-1` and `-2` placeholders in the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data.replace([-2, '-2', -1, '-1'], 0, inplace=True)"
   ]
  },
  {
   "source": [
    "#### Remove unnecessary uniqueness columns from the data set, and check for any `NaN` values. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnecessary_uniqueness_columns = ['INSTNM', 'IALIAS', 'FIPS', 'OBEREG', 'GENTELE', 'EIN', 'DUNS', 'OPEID', 'CNGDSTCD']\n",
    "df_all_data.drop(unnecessary_uniqueness_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_not_null = df_all_data.isnull().sum().sum()\n",
    "col_count = df_all_data.shape[1]\n",
    "row_count = df_all_data.shape[0]\n",
    "\n",
    "print(f\"The size of the data is {col_count * row_count} with {data_not_null} NaN items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_null = df_all_data.isnull().sum().to_frame('NaN')\n",
    "df_data_null[df_data_null['NaN'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_nulls_with_column_median(list_of_columns, dataframe):\n",
    "\n",
    "    \"\"\"\n",
    "    The function takes a list of applicable columns and a corresponding dataframe. \n",
    "    The median is calculated per column and then identifies NaNs to be replaced with the median\n",
    "\n",
    "    INPUT: \n",
    "\n",
    "    list_of_columns: a list of columns with null values\n",
    "    dataframe: the dataframe relating to the specified list.\n",
    "\n",
    "    OUTPUT:\n",
    "\n",
    "    None\n",
    "        The dataframe values are replaced inplace to there is no need to return the frame\n",
    "    \"\"\"\n",
    "\n",
    "    for col in list_of_columns:\n",
    "        impute_median = int(dataframe[col].median())\n",
    "        dataframe[col].fillna(value=impute_median, inplace=True)\n",
    "    \n",
    "    df_dataframe_nulls = dataframe.isnull().sum().to_frame('Nulls')\n",
    "    list_null_cols_vals = df_dataframe_nulls[df_dataframe_nulls['Nulls'] > 0]\n",
    "    \n",
    "    print(f'\\nImputing of values complete, you can find the df header below: \\n\\n {dataframe.head()}')\n",
    "    print(f'\\nLets look for the amount of nulls remaining in the dataframe: \\n\\n')\n",
    "\n",
    "    for rows in list_null_cols_vals.items():\n",
    "        print(f'{rows} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_null_data = list(df_data_null[df_data_null['NaN'] > 0].index)\n",
    "impute_nulls_with_column_median(columns_with_null_data, df_all_data)"
   ]
  },
  {
   "source": [
    "#### Now that we've improved the data quality by removing the `NaN` values we need to standardise the values for the KNN model. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = df_all_data.select_dtypes(include=['object'])\n",
    "list_of_categorical_columns = list(categorical_columns.columns)\n",
    "target_classifier = ['CONVERTED']\n",
    "unique_id = ['UNITID']\n",
    "list_of_columns_to_drop = list_of_categorical_columns + target_classifier + unique_id"
   ]
  },
  {
   "source": [
    "#### Split the data into 3 distinct sets: \n",
    "* All data\n",
    "* All data of where organisations have been contaced\n",
    "* All data of organisations that haven't been contacted\n",
    "\n",
    "The write the data to the sqlite dB"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data_contacted = df_all_data[df_all_data['CONTACTED'] == 1]\n",
    "df_all_data_contacted.to_sql('all_data_contacted', engine, if_exists='replace', index=False)\n",
    "df_all_data_not_contacted = df_all_data[df_all_data['CONTACTED'] == 0]\n",
    "df_all_data_not_contacted.to_sql('all_data_not_contacted', engine, if_exists='replace', index=False)\n",
    "#also write the df_all_data frame to sql\n",
    "df_all_data.to_sql('all_data', engine, if_exists='replace', index=False)"
   ]
  },
  {
   "source": [
    "#### Standardise the data using the `StandardScalar()` from `sklearn`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scalar_all_data_contacted = df_all_data_contacted.drop(list_of_columns_to_drop, axis = 1)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_scalar_all_data_contacted)\n",
    "scaled_features = scaler.transform(df_scalar_all_data_contacted)\n",
    "df_feat_all_contacted = pd.DataFrame(scaled_features, columns=df_scalar_all_data_contacted.columns)\n",
    "df_feat_all_contacted.to_sql('feat_all_contacted', engine, if_exists='replace', index=False)\n",
    "df_feat_all_contacted.head()\n"
   ]
  }
 ]
}